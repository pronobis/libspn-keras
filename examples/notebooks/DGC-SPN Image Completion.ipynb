{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cw8l-b_NIaBy"
   },
   "source": [
    "# Training a Deep Generalized Convolutional Sum-Product Network (DGC-SPN) for image completion.\n",
    "Let's go through an example of building complex SPNs with [`libspn-keras`](https://github.com/pronobis/libspn-keras). The layer-based API of the library makes it straightforward to build advanced SPN architectures.\n",
    "\n",
    "First let's set up the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZjJ0ZvPiGV3"
   },
   "outputs": [],
   "source": [
    "!pip install libspn-keras matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "We'll use the Olivetti faces dataset. Note that this dataset is small by today's standards. Nevertheless, we'll be able to produce pretty good completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/pronobis/libspn-keras/master/examples/notebooks/olivetti.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b2_oxHcOh3Rk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_olivetti(test_size=50):\n",
    "    x = np.loadtxt(\"olivetti.raw\").transpose().reshape(\n",
    "        400, 64, 64).transpose((0, 2, 1)).astype(np.float32)\n",
    "    train_x = x[:-test_size]\n",
    "    test_x = x[-test_size:]\n",
    "    return train_x, test_x\n",
    "\n",
    "train_x, test_x = load_olivetti()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Reasonable Default for Accumulator Initializers\n",
    "Since we're going to be using hard EM-based learning that will automatically break symmetries for equally weighted sums in the SPN, \n",
    "we'll use a default accumulator initializer that initializes all weights with \n",
    "$$\\epsilon \\cdot \\text{fan_in}^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libspn_keras as spnk\n",
    "\n",
    "spnk.set_default_accumulator_initializer(\n",
    "    spnk.initializers.EpsilonInverseFanIn(epsilon=1e-4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Variables\n",
    "For the leaf variable, we'll use `spnk.layers.NormalLeaf`. We initialize the locations of the normal distribution components through an initializer that was based on the seminal work by [Poon and Domingos (2011)](https://arxiv.org/abs/1202.3732). This splits up train data into $n$ quantiles per pixel, after which\n",
    "the mean of the $i$-th quantile is used as the location of the $i$-th component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_epsilon = 1e-8\n",
    "\n",
    "location_initializer = spnk.initializers.PoonDomingosMeanOfQuantileSplit(\n",
    "    data=train_x.squeeze(), normalization_epsilon=normalization_epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hM7gCCcdfJ3U"
   },
   "source": [
    "## DGC-SPN\n",
    "A DGC-SPN consists of convolutional product and sum nodes. \n",
    "\n",
    "We begin by using non-overlapping convolution patches for our first two product layers,\n",
    "since this way we make sure that no scopes overlap between products. \n",
    "\n",
    "For all convolutional product layers, we'll use *exponentially increasing dilation rates*. By doing so, we have 'overlapping' patches that still yield a valid SPN. These exponentially increasing dilation rates for convolutional SPNs were first introduced in [this paper](https://arxiv.org/abs/1902.06155).\n",
    "\n",
    "The layer SPN construction is wrapped in a function so that we can redefine it later for other purposes. Don't worry, we'll get to the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aADN6sXCzrea"
   },
   "outputs": [],
   "source": [
    "def define_spn(sum_op, infer_no_evidence=False):\n",
    "    spnk.set_default_sum_op(sum_op)\n",
    "    stack = [\n",
    "        spnk.layers.NormalizeStandardScore(input_shape=(64, 64, 1)),\n",
    "        # Non-overlapping products\n",
    "        spnk.layers.NormalLeaf(\n",
    "            num_components=4, \n",
    "            location_trainable=False,\n",
    "            location_initializer=location_initializer,\n",
    "            use_accumulators=True,\n",
    "            scale_trainable=False\n",
    "        ),\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=True, \n",
    "            strides=[1, 1], \n",
    "            dilations=[1, 1], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='full'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        # Non-overlapping products\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=False, \n",
    "            strides=[1, 1], \n",
    "            dilations=[2, 2], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='full'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        # Overlapping products, starting at dilations [1, 1]\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=False, \n",
    "            strides=[1, 1], \n",
    "            dilations=[4, 4], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='full'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        # Overlapping products, with dilations [2, 2] and full padding\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=False, \n",
    "            strides=[1, 1], \n",
    "            dilations=[8, 8], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='full'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        # Overlapping products, with dilations [2, 2] and full padding\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=False, \n",
    "            strides=[1, 1], \n",
    "            dilations=[16, 16], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='full'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=False, \n",
    "            strides=[1, 1], \n",
    "            dilations=[32, 32], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='full'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        # Overlapping products, with dilations [2, 2] and 'final' padding to combine \n",
    "        # all scopes\n",
    "        spnk.layers.Conv2DProduct(\n",
    "            depthwise=False, \n",
    "            strides=[1, 1], \n",
    "            dilations=[64, 64], \n",
    "            kernel_size=[2, 2],\n",
    "            padding='final'\n",
    "        ),\n",
    "        spnk.layers.Local2DSum(num_sums=2),\n",
    "        spnk.layers.SpatialToRegions(),\n",
    "        spnk.layers.RootSum(\n",
    "            return_weighted_child_logits=False\n",
    "        )\n",
    "    ]\n",
    "    sum_product_network = spnk.models.SequentialSumProductNetwork(\n",
    "      stack, infer_no_evidence=infer_no_evidence)\n",
    "    return sum_product_network\n",
    "\n",
    "sum_product_network = define_spn(sum_op=spnk.SumOpUnweightedHardEMBackprop())\n",
    "sum_product_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXN945AmcvNh"
   },
   "source": [
    "### Training the DGC-SPN with hard EM\n",
    "We use the `libspn_keras.optimizers.OnlineExpectationMaximization` optimizer. Note that this optimizer works for generative training setups only and should be\n",
    "combined with one of the EM-based sum ops. In the snippet above, we have set this to ``SumOpUnweightedEMBackprop``, so we're good to go!\n",
    "\n",
    "We'll configure a train set and a test set using `tf.data.Dataset`. As first suggested in the work by [Poon and Domingos](https://arxiv.org/abs/1202.3732), we normalize each sample by subtracting the mean and dividing by the\n",
    "standard devation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wjmtjHUvct-q"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from libspn_keras.optimizers import OnlineExpectationMaximization\n",
    "from libspn_keras import losses\n",
    "from libspn_keras import metrics\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_data = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_x,))\n",
    "    .shuffle(350)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_x,))\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "sum_product_network.compile(\n",
    "    optimizer=spnk.optimizers.OnlineExpectationMaximization(), \n",
    "    loss=spnk.losses.NegativeLogLikelihood(),\n",
    "    metrics=[spnk.metrics.LogLikelihood()]\n",
    ")\n",
    "sum_product_network.fit(train_data, epochs=10)\n",
    "sum_product_network.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_3_y8fe5IT_"
   },
   "source": [
    "Now comes the tricky part. To do image completion, we need to propagate some kind of signal to the leafs that we occlude. Probabilistically, we exclude the variables corresponding to the occluded pixels from the evidence $\\mathbf E$.\n",
    "\n",
    "In the seminal work by [Poon and Domingos (2011)](https://arxiv.org/abs/1202.3732), in which Sum-Product Networks were proposed as a new type of deep probabilistic models, the image completion was accomplished by backpropagating the **gradients** and using the these to obtain the 'posterior marginal' of the missing variables.\n",
    "\n",
    "The paper itself, however, does not mention this explicitly. Only after we inspect the code from the paper that comes with it we find:\n",
    "```java\n",
    "// compute marginal by differentiation; see Darwiche-03 for details \n",
    "public void cmpMAPBottomHalfMarginal(Instance inst) {\n",
    "  setInputOccludeBottomHalf(inst);\n",
    "  eval();\t\t\n",
    "  cmpDerivative();\n",
    "      \n",
    "  for (int i=0; i<Parameter.inputDim1_/2; i++) {\n",
    "    for (int j=0; j<Parameter.inputDim2_; j++) \n",
    "      MyMPI.buf_int_[MyMPI.buf_idx_++]=Utils.getIntVal(inst, inst.vals_[i][j]);\n",
    "  }\n",
    "  for (int i=Parameter.inputDim1_/2; i<Parameter.inputDim1_; i++) {\n",
    "    for (int j=0; j<Parameter.inputDim2_; j++) {\n",
    "      int ri=Region.getRegionId(i, i+1, j, j+1);\n",
    "      Region r=Region.getRegion(ri);\n",
    "      double p=cmpMarginal(r);\n",
    "      MyMPI.buf_int_[MyMPI.buf_idx_++]=Utils.getIntVal(inst,p);//(int)(p*255);\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "Note the commpent on top! [The paper by Darwiche (2003)](http://reasoning.cs.ucla.edu/fetch.php?id=22&type=pdf) indeed mentions a way of determining the _posterior marginal_ of any variable when the network computes a _network polynomial_. SPNs happen to compute a network polynomial, so we can indeed use gradients to compute posterior marginals!\n",
    "\n",
    "From the paper, we have:\n",
    "\n",
    "_For every variable $X$ and evidence $\\mathbf e \\notin \\mathbf E$_:\n",
    "$$\n",
    "P(x_i \\mid \\mathbf e) = \\frac{1}{\\text{Root}(\\mathbf e)} \\frac{\\partial \\text{Root}}{\\partial \\text{Leaf}_{x_i}}(\\mathbf e)\n",
    "$$\n",
    "\n",
    "Note that in the last equation we are using lower case variable with an index: $x_i$. The index corresponds to the _component_ on top of the variable. In other words, $P(x_i \\mid \\mathbf e)$ is the posterior marginal of the $i$-th component attached to variable $X$. The variable $X$ is excluded from the evidence $\\mathbf E$. Lower case $\\mathbf e$ is the actual assignment of pixel values in the part of the image that is _included_ (not occluded). Note that $P(x_i \\mid \\mathbf e)$ is **not** the output probability of the leaf. Rather, it is the probability that the pixel value at $X$ was _generated_ by the component at $x_i$.\n",
    "\n",
    "In the code by Poon and Domingos we mentioned earlier, the posterior marginals are used as follows:\n",
    "$$\n",
    "\\text{InferredPixelValue} = \\sum_i \\mu_i P(x_i \\mid \\mathbf e)\n",
    "$$\n",
    "Where $\\mu_i$ is the mean of the $i$-th Gaussian component.\n",
    "\n",
    "So how would this work for the SPN we have now? The layer implementations in LibSPN Keras are propagating log probabilities. So at the root of the network we'll find $\\log(\\text{Root}(\\mathbf e))$ instead of just $\\text{Root}(\\mathbf e)$. Simply wrapping the value with $\\exp(\\cdot)$ results in numerical difficulties, so that's a dead end. \n",
    "\n",
    "Let's reverse our approach: we'll start propagating gradients from the roots to the leafs and see how far we are from solving the problem!\n",
    "\n",
    "If we just use TensorFlow's autograd engine we can obtain something like:\n",
    "```python\n",
    "leaf_log_prob = leaf(img)\n",
    "leaf_log_prob = tf.where(evidence_mask, leaf_log_prob, tf.zeros_like(leaf_log_prob))\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(leaf_log_prob)\n",
    "    root = apply_sum_product_stack(leaf_log_prob)\n",
    "droot_log_prob_dleaf_log_prob = g.gradient(root_log_prob, leaf_log_prob)\n",
    "```\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log(\\text{Root})}{\\partial \\log(\\text{Leaf}_{x_i})}\n",
    "$$\n",
    "\n",
    "Which we can rewrite a bit:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\log(\\text{Root})}{\\partial \\log(\\text{Leaf}_{x_i})}\n",
    "&=\\frac{\\partial \\log(\\text{Root})}{\\partial \\text{Root}}\n",
    "\\frac{\\partial \\text{Root}}{\\partial \\log(\\text{Leaf}_{x_i})} \\\\&= \\frac{1}{Root} \\frac{\\partial \\text{Root}}{\\partial \\log(\\text{Leaf}_{x_i})} \\\\ &= \\frac{1}{Root} \\frac{\\partial \\text{Root}}{\\partial \\log(\\text{Leaf}_{x_i})} \\frac{1}{\\text{Leaf}_{x_i}} \\text{Leaf}_{x_i} \\\\&=  \\frac{1}{Root} \\frac{\\partial \\text{Root}}{\\partial \\log(\\text{Leaf}_{x_i})} \\frac{\\partial \\log (\\text{Leaf}_{x_i})}{\\partial \\text{Leaf}_{x_i}} \\text{Leaf}_{x_i} \\\\&=  \\frac{1}{Root} \\frac{\\partial \\text{Root}}{\\partial \\text{Leaf}_{x_i}}  \\text{Leaf}_{x_i} \\\\\n",
    "&=\\frac{1}{Root} \\frac{\\partial \\text{Root}}{\\partial \\text{Leaf}_{x_i}} & (x_i \\notin \\mathbf E\\text{, so } \\text{Leaf}_{x_i} = 1)\\\\\n",
    "&= P(x_i \\mid \\mathbf e)\n",
    "\\end{align}\n",
    "\n",
    "So in fact, we already have what we want in `droot_log_prob_dleaf_log_prob`!\n",
    "\n",
    "Below, we use those observations to compute the image completions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WQqVxTyr2V1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "completion_model = define_spn(\n",
    "    sum_op=spnk.SumOpGradBackprop(logspace_accumulators=False), \n",
    "    infer_no_evidence=True\n",
    ")\n",
    "completion_model.set_weights(sum_product_network.get_weights())\n",
    "completion_model.compile(metrics=[keras.metrics.MeanSquaredError()])\n",
    "\n",
    "test_x_numpy = np.array([d[0] for d in test_data.as_numpy_iterator()])[0]\n",
    "\n",
    "def eval(model, x, omit_side):\n",
    "    print(\"omitting \", omit_side)\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    evidence_mask = get_evidence_mask(omit_side).astype(np.bool)\n",
    "    model.evaluate([x, evidence_mask], x, verbose=2)\n",
    "    completion_out = model.predict([x, evidence_mask])\n",
    "    image_grid = make_image_grid(completion_out, num_rows=5)\n",
    "\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    plt.imshow(image_grid.squeeze(), cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "def make_image_grid(images, num_rows):\n",
    "    images_per_row = np.split(images, axis=0, indices_or_sections=num_rows)\n",
    "    rows = [np.concatenate(imgs, axis=1) for imgs in images_per_row]\n",
    "    full_grid = np.concatenate(rows, axis=0)\n",
    "    return full_grid\n",
    "\n",
    "\n",
    "def get_evidence_mask(omit_side):\n",
    "  if omit_side == \"top\":\n",
    "    return np.concatenate(\n",
    "        [np.zeros([50, 32, 64, 1]), np.ones([50, 32, 64, 1])], axis=1)\n",
    "  elif omit_side == 'bottom':\n",
    "    return np.concatenate(\n",
    "        [np.ones([50, 32, 64, 1]), np.zeros([50, 32, 64, 1])], axis=1)\n",
    "  elif omit_side == 'right':\n",
    "    return np.concatenate(\n",
    "        [np.ones([50, 64, 32, 1]), np.zeros([50, 64, 32, 1])], axis=2)\n",
    "  elif omit_side == 'left':\n",
    "    return np.concatenate(\n",
    "        [np.zeros([50, 64, 32, 1]), np.ones([50, 64, 32, 1])], axis=2)\n",
    "  else:\n",
    "    raise ValueError(\"We have a problem\")\n",
    "\n",
    "eval(completion_model, test_x_numpy, 'top')\n",
    "eval(completion_model, test_x_numpy, 'bottom')\n",
    "eval(completion_model, test_x_numpy, 'left')\n",
    "eval(completion_model, test_x_numpy, 'right')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "libspn-keras DGC-SPN image completion tutorial v0.3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
